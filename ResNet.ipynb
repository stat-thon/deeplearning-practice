{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "# CIFAR-10 데이터셋 다운로드 및 로드\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다운이 코드 참고해서 block화 읽기\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        # __init__ 메소드 안에 함수를 정의하기도 하는군\n",
    "        def block(in_channels_, out_channels, kernel_size = 3, padding = 1):\n",
    "            \n",
    "            # downsampling 할 때는 stride가 2\n",
    "            if in_channels == out_channels:\n",
    "                stride = 1\n",
    "            \n",
    "            else:\n",
    "                stride = 2\n",
    "                \n",
    "            layer = nn.Sequential(\n",
    "                \n",
    "                nn.Conv2d(in_channels = in_channels_, out_channels = out_channels, kernel_size = kernel_size,\n",
    "                          stride = stride, padding = padding),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU()\n",
    "                \n",
    "                nn.Conv2d(in_channels = out_channels, out_channels = out_channels, kernel_size = kernel_size,\n",
    "                          stride = stride, padding = padding)\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "                \n",
    "            )\n",
    "            \n",
    "            return layer\n",
    "        \n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size = 7, )\n",
    "                \n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다운이 코드\n",
    "#%%writefile \"../model/ResNet_Residual.py\"\n",
    "\n",
    "# import modules\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "class ResNet_Residual(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        # super함수는 class의 부모 class인 nn.Module을 초기화\n",
    "        super(ResNet_Residual, self).__init__()\n",
    "\n",
    "        # Plain이나 Resnet이나 블록 형태는 같다. \n",
    "        def block(in_channels, out_channels, kernel_size = 3, padding = 1):\n",
    "            # downsampling할 때 stride 가 2\n",
    "            if in_channels == out_channels:\n",
    "                stride = 1\n",
    "            \n",
    "            else:\n",
    "                stride = 2\n",
    "\n",
    "            # layer block -> ResNet에서는 이부분이 Residual block이 될 예정\n",
    "            layer = nn.Sequential(\n",
    "                nn.Conv2d(in_channels = in_channels, out_channels = out_channels, kernel_size = kernel_size, stride = stride, padding = padding),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.Relu(),\n",
    "\n",
    "                nn.Conv2d(in_channels = out_channels, out_channels = out_channels, kernel_size = kernel_size, padding = padding),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "            return layer\n",
    "\n",
    "        # 맨 처음 7x7과 풀링(VGG19에서 Maxpooling 사용)\n",
    "        self.seven_conv = nn.Conv2d(in_channels = 3, out_channels = 64, padding = 3)\n",
    "        self.pool = nn.MaxPool2d(kernel_size = 2)\n",
    "        \n",
    "        # 64층\n",
    "        self.block64_1 = block(in_channels = 64, out_channels = 64)\n",
    "        self.block64_2 = block(in_channels = 64, out_channels = 64)\n",
    "        self.block64_3 = block(in_channels = 64, out_channels = 64)\n",
    "        \n",
    "        # 128층\n",
    "        self.block128_1 = block(in_channels = 64, out_channels = 128)\n",
    "        self.block128_2 = block(in_channels = 128, out_channels = 128)\n",
    "        self.block128_3 = block(in_channels = 128, out_channels = 128)\n",
    "        self.block128_4 = block(in_channels = 128, out_channels = 128)\n",
    "        \n",
    "        # 256층\n",
    "        self.block256_1 = block(in_channels = 128, out_channels = 256)\n",
    "        self.block256_2 = block(in_channels = 256, out_channels = 256)\n",
    "        self.block256_3 = block(in_channels = 256, out_channels = 256)\n",
    "        self.block256_4 = block(in_channels = 256, out_channels = 256)\n",
    "        self.block256_5 = block(in_channels = 256, out_channels = 256)\n",
    "        self.block256_6 = block(in_channels = 256, out_channels = 256)\n",
    "        \n",
    "        # 512층\n",
    "        self.block512_1 = block(in_channels = 256, out_channels = 512)\n",
    "        self.block512_2 = block(in_channels = 512, out_channels = 512)\n",
    "        self.block512_3 = block(in_channels = 512, out_channels = 512)\n",
    "\n",
    "        # avgpool\n",
    "        self.average_pool = nn.AdaptiveAvgPool2d(kernel_size = 1)\n",
    "\n",
    "        # fully connected layer\n",
    "        self.fully_connected_layer = nn.Linear(in_channels = 512, out_channels = 1000)\n",
    "\n",
    "        # 추가 relu\n",
    "        self.relu = nn.Relu()\n",
    "\n",
    "        # 보정\n",
    "        self.prejection1 = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 1)\n",
    "        self.prejection2 = nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 1)\n",
    "        self.prejection3 = nn.Conv2d(in_channels = 256, out_channels = 512, kernel_size = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # plain -> 단순 연결만 진행하면 됨\n",
    "        x = self.seven_conv(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # 64\n",
    "        res1 = self.block64_1(x)\n",
    "        x = res1 + x\n",
    "        x = self.relu(x)\n",
    "\n",
    "        res2 = self.block64_2(x)\n",
    "        x = res2 + x\n",
    "        x = self.relu(x)\n",
    "\n",
    "        res3 = self.block64_3(x)\n",
    "        x = res3 + x\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # 128\n",
    "        res1 = self.block128_1(x)\n",
    "        x = self.prejection1(x)\n",
    "        x = res1 + x\n",
    "        x = self.relu(x)\n",
    "\n",
    "        res2 = self.block128_2(x)\n",
    "        x = res2 + x\n",
    "        x = self.relu(x)\n",
    "\n",
    "        res3 = self.block128_3(x)\n",
    "        x = res3 + x\n",
    "        x = self.relu(x)\n",
    "\n",
    "        res4 = self.block128_4(x)\n",
    "        x = res4 + x\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # 256\n",
    "        res1 = self.block256_1(x)\n",
    "        x = self.prejection2(x)\n",
    "        x = res1 + x\n",
    "        x = self.relu(x)\n",
    "\n",
    "        res2 = self.block256_2(x)\n",
    "        x = res2 + x\n",
    "        x = self.relu(x)\n",
    "\n",
    "        res3 = self.block256_3(x)\n",
    "        x = res3 + x\n",
    "        x = self.relu(x)\n",
    "\n",
    "        res4 = self.block256_4(x)\n",
    "        x = res4 + x\n",
    "        x = self.relu(x)\n",
    "\n",
    "        res5 = self.block256_5(x)\n",
    "        x = res5 + x\n",
    "        x = self.relu(x)\n",
    "\n",
    "        res6 = self.block256_6(x)\n",
    "        x = res6 + x\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # 512\n",
    "        res1 = self.block512_1(x)\n",
    "        x = self.prejection3(x)\n",
    "        x = res1 + x\n",
    "        x = self.relu(x)\n",
    "\n",
    "        res2 = self.block512_2(x)\n",
    "        x = res2 + x\n",
    "        x = self.relu(x)\n",
    "\n",
    "        res3 = self.block512_3(x)\n",
    "        x = res3 + x\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # 마지막 결과물\n",
    "        x = self.average_pool(x)\n",
    "        x = self.fully_connected_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상혁 코드\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.stride = stride\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(self.in_channels, self.out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(self.out_channels), # 학습 빠르게 하기 위함\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(self.out_channels) # 학습 빠르게 하기 위함\n",
    "        )\n",
    "        \n",
    "        if self.stride != 1 or self.in_channels != self.out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, self.out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.out_channels)\n",
    "            )\n",
    "            \n",
    "    # Block 하나 거칠 때마다 이미지 사이즈가 줄고, 채널 수 늘어나는 구조\n",
    "    # 처음 들어오는 X와 Block을 거친 출력값 out의 크기가 같아야함.\n",
    "    # X, out의 크기가 차이가 날 경우 크기를 동일하게 하기 위해 별도 conv 연산을 진행하여\n",
    "    # 입력 크기와 출력 크기를 같게 맞춤 \n",
    "    def forward(self, x):\n",
    "        out = self.conv_block(x)\n",
    "        if self.stride != 1 or self.in_channels != self.out_channels:\n",
    "            x = self.downsample(x)\n",
    "        out = F.relu(x + out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.base = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # cifar 10의 경우 32 x 32 \n",
    "        self.layer1 = self.make_layer(64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self.make_layer(128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self.make_layer(256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self.make_layer(512, num_blocks[3], stride=2)\n",
    "\n",
    "        self.gap = nn.AvgPool2d(4)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "\n",
    "    def make_layer(self, out_channels, num_block, stride):\n",
    "        strides = [stride] + [1] * (num_block - 1)\n",
    "        layers = []\n",
    "        \n",
    "        for stride in strides:\n",
    "            block = ResidualBlock(self.in_channels, out_channels, stride)\n",
    "            layers.append(block)\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.base(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.gap(out)\n",
    "\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "def modeltype(model):\n",
    "    if model == 'resnet18':\n",
    "        return ResNet([2, 2, 2, 2])\n",
    "    elif model == 'resnet34':\n",
    "        return ResNet([3, 4, 6, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 준혁 코드\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class res_net():\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(res_net, self).__init__()\n",
    "        \n",
    "        def CBR2d_Blocks(first_ch, inout_ch, kernel_size=3, stride=1, padding=1, bias=True):\n",
    "            return nn.Sequential(nn.Conv2d(in_channels=first_ch, out_channels=inout_ch, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
    "                      nn.BatchNorm2d(inout_ch),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Conv2d(in_channels=inout_ch, out_channels=inout_ch, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
    "                      nn.BatchNorm2d(inout_ch),\n",
    "                      nn.ReLU())\n",
    "\n",
    "        self.first_layer = nn.Sequential(nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=0, bias=True),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Maxpool2d(kernel_size=2),\n",
    "                                         nn.ReLU()\n",
    "                                         )\n",
    "        \n",
    "\n",
    "        self.second_layer = CBR2d_Blocks(first_ch=64, inout_ch=64)\n",
    "        self.to_third_layer = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=1, stride=2, padding=0, bias=True)\n",
    "\n",
    "        self.third_layer_1 = CBR2d_Blocks(first_ch=64, inout_ch=128)\n",
    "        self.third_layer_2 = CBR2d_Blocks(first_ch=128, inout_ch=128)\n",
    "        self.to_fourth_layer = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=1, stride=2, padding=0, bias=True)\n",
    "      \n",
    "\n",
    "        self.forth_layer_1 = CBR2d_Blocks(fisrt_ch=128, inout_ch=256)\n",
    "        self.forth_layer_2 = CBR2d_Blocks(first_ch=256, inout_ch=256)\n",
    "        self.to_fifth_layer = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=1, stride=2, padding=0, bias=True)\n",
    "\n",
    "        self.fifth_layer_1 = CBR2d_Blocks(first_ch=256, inout_ch=512)\n",
    "        self.fifth_layer_2 = CBR2d_Blocks(first_ch=512, inout_ch=512)\n",
    "\n",
    "\n",
    "        self.last_layer = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # first_layer\n",
    "        x = self.first_layer(x)\n",
    "\n",
    "        # 64 channels\n",
    "        identity = x\n",
    "        x = self.second_layer(x)\n",
    "        x += identity\n",
    "        x = nn.ReLU(x)\n",
    "        identity = x\n",
    "\n",
    "        x = self.second_layer(x)\n",
    "        x += identity\n",
    "        x = nn.ReLU(x)\n",
    "        identity = x\n",
    "        \n",
    "        x = self.second_layer(x)\n",
    "        x += identity\n",
    "        x = nn.ReLU(x)\n",
    "        identity = x\n",
    "\n",
    "        # identity to next layer\n",
    "        identity = self.to_third_layer(identity)\n",
    "\n",
    "        # 128 channels\n",
    "        x = self.third_layer_1(x)\n",
    "        x += identity\n",
    "        x = nn.ReLU(x)\n",
    "        identity = x\n",
    "\n",
    "        x = self.third_layer_2(x)\n",
    "        x += identity\n",
    "        x = nn.ReLU(x)\n",
    "        identity = x\n",
    "\n",
    "        x = self.third_layer_2(x)\n",
    "        x += identity\n",
    "        x = nn.ReLU(x)\n",
    "        identity = x\n",
    "\n",
    "        x = self.third_layer_2(x)\n",
    "        x += identity\n",
    "        x = nn.ReLU(x)\n",
    "        identity = x\n",
    "\n",
    "        # identity to next layer\n",
    "        identity = self.to_fifth_layer(identity)\n",
    "\n",
    "        # 256 channels\n",
    "        x = self.fourth_layer_1(x)\n",
    "        x += identity\n",
    "        x = nn.ReLU(x)\n",
    "        identity = x\n",
    "\n",
    "        x = self.fourth_layer_2(x)\n",
    "        x += identity\n",
    "        x = nn.ReLU(x)\n",
    "        identity = x\n",
    "\n",
    "        x = self.fourth_layer_2(x)\n",
    "        x += identity\n",
    "        x = nn.ReLU(x)\n",
    "        identity = x\n",
    "\n",
    "        x = self.fourth_layer_2(x)\n",
    "        x += identity\n",
    "        x = nn.ReLU(x)\n",
    "        identity = x\n",
    "\n",
    "        x = self.fourth_layer_2(x)\n",
    "        x += identity\n",
    "        x = nn.ReLU(x)\n",
    "        identity = x\n",
    "\n",
    "        x = self.fourth_layer_2(x)\n",
    "        x += identity\n",
    "        x = nn.ReLU(x)\n",
    "        identity = x\n",
    "\n",
    "        # identity to next layer\n",
    "        identity = self.to_fifth_layer(identity)\n",
    "\n",
    "        # 512 channels\n",
    "        x = self.fifth_layer_1(x)\n",
    "        x += identity\n",
    "        x = nn.ReLU(x)\n",
    "        identity = x\n",
    "\n",
    "        x = self.fifth_layer_2(x)\n",
    "        x += identity\n",
    "        x = nn.ReLU(x)\n",
    "        identity = x\n",
    "\n",
    "        x = self.fifth_layer_2(x)\n",
    "        x += identity\n",
    "        x = nn.ReLU(x)\n",
    "        identity = x\n",
    "\n",
    "        # last_layer\n",
    "        x = self.last_layer(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ResNet.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"ResNet.py\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init_()\n",
    "        \n",
    "        # filter size 7\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size = 7, stride = 2, padding = 3) # in_channels self.parameter로 추가하는 게 낫지 않을까\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size = 3, stride = 2)\n",
    "        \n",
    "        # 64 channels\n",
    "        self.conv2_1 = nn.Conv2d(64, 64, kernel_size = 3, padding = 1) # stride default = 1\n",
    "        self.bn2_1 = nn.BatchNorm2d(64)\n",
    "        self.relu2_1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2_2 = nn.Conv2d(64, 64, kernel_size = 3, padding = 1)\n",
    "        self.bn2_2 = nn.BatchNorm2d(64)\n",
    "        self.relu2_2 = nn.ReLU()\n",
    "        \n",
    "        self.conv2_3 = nn.Conv2d(64, 64, kernel_size = 3, padding = 1)\n",
    "        self.bn2_3 = nn.BatchNorm2d(64)\n",
    "        self.relu2_3 = nn.ReLU()\n",
    "        \n",
    "        self.conv2_4 = nn.Conv2d(64, 64, kernel_size = 3, padding = 1)\n",
    "        self.bn2_4 = nn.BatchNorm2d(64)\n",
    "        self.relu2_4 = nn.ReLU()\n",
    "        \n",
    "        self.conv2_5 = nn.Conv2d(64, 64, kernel_size = 3, padding = 1)\n",
    "        self.bn2_5 = nn.BatchNorm2d(64)\n",
    "        self.relu2_5 = nn.ReLU()\n",
    "        \n",
    "        self.conv2_6 = nn.Conv2d(64, 64, kernel_size = 3, padding = 1)\n",
    "        self.bn2_6 = nn.BatchNorm2d(64)\n",
    "        self.relu2_6 = nn.ReLU()\n",
    "        \n",
    "        # 128 channels\n",
    "        self.conv3_1 = nn.Conv2d(64, 128, kernel_size = 3, stride = 2)\n",
    "        self.bn3_1 = nn.BatchNorm2d(128)\n",
    "        self.relu3_1 = nn.ReLU()\n",
    "        \n",
    "        self.conv3_2 = nn.Conv2d(128, 128, kernel_size = 3, padding = 1)\n",
    "        self.bn3_2 = nn.BatchNorm2d(128)\n",
    "        self.relu3_2 = nn.ReLU()\n",
    "        \n",
    "        self.conv3_proj = nn.Conv2d(64, 128, kernel_size = 1) # for projection shortcut\n",
    "        \n",
    "        self.conv3_3 = nn.Conv2d(128, 128, kernel_size = 3, padding = 1)\n",
    "        self.bn3_3 = nn.BatchNorm2d(128)\n",
    "        self.relu3_3 = nn.ReLU()\n",
    "        \n",
    "        self.conv3_4 = nn.Conv2d(128, 128, kernel_size = 3, padding = 1)\n",
    "        self.bn3_4 = nn.BatchNorm2d(128)\n",
    "        self.relu3_4 = nn.ReLU()\n",
    "        \n",
    "        self.conv3_5 = nn.Conv2d(128, 128, kernel_size = 3, padding = 1)\n",
    "        self.bn3_5 = nn.BatchNorm2d(128)\n",
    "        self.relu3_5 = nn.ReLU()\n",
    "        \n",
    "        self.conv3_6 = nn.Conv2d(128, 128, kernel_size = 3, padding = 1)\n",
    "        self.bn3_6 = nn.BatchNorm2d(128)\n",
    "        self.relu3_6 = nn.ReLU()\n",
    "        \n",
    "        self.conv3_7 = nn.Conv2d(128, 128, kernel_size = 3, padding = 1)\n",
    "        self.bn3_7 = nn.BatchNorm2d(128)\n",
    "        self.relu3_7 = nn.ReLU()\n",
    "        \n",
    "        self.conv3_8 = nn.Conv2d(128, 128, kernel_size = 3, padding = 1)\n",
    "        self.bn3_8 = nn.BatchNorm2d(128)\n",
    "        self.relu3_8 = nn.ReLU()\n",
    "        \n",
    "        # 256 channels\n",
    "        self.conv4_1 = nn.Conv2d(128, 256, kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.bn4_1 = nn.BatchNorm2d(256)\n",
    "        self.relu4_1 = nn.ReLU()\n",
    "        \n",
    "        self.conv4_2 = nn.Conv2d(256, 256, kernel_size = 3, padding = 1)\n",
    "        self.bn4_2 = nn.BatchNorm2d(256)\n",
    "        self.relu4_2 = nn.ReLU()\n",
    "        \n",
    "        self.conv4_proj = nn.Conv2d(128, 256, kernel_size = 1) # for projection shortcut\n",
    "        \n",
    "        self.conv4_3 = nn.Conv2d(256, 256, kernel_size = 3, padding = 1)\n",
    "        self.bn4_3 = nn.BatchNorm2d(256)\n",
    "        self.relu4_3 = nn.ReLU()\n",
    "        \n",
    "        self.conv4_4 = nn.Conv2d(256, 256, kernel_size = 3, padding = 1)\n",
    "        self.bn4_4 = nn.BatchNorm2d(256)\n",
    "        self.relu4_4 = nn.ReLU()\n",
    "        \n",
    "        self.conv4_5 = nn.Conv2d(256, 256, kernel_size = 3, padding = 1)\n",
    "        self.bn4_5 = nn.BatchNorm2d(256)\n",
    "        self.relu4_5 = nn.ReLU()\n",
    "        \n",
    "        self.conv4_6 = nn.Conv2d(256, 256, kernel_size = 3, padding = 1)\n",
    "        self.bn4_6 = nn.BatchNorm2d(256)\n",
    "        self.relu4_6 = nn.ReLU()\n",
    "        \n",
    "        self.conv4_7 = nn.Conv2d(256, 256, kernel_size = 3, padding = 1)\n",
    "        self.bn4_7 = nn.BatchNorm2d(256)\n",
    "        self.relu4_7 = nn.ReLU()\n",
    "        \n",
    "        self.conv4_8 = nn.Conv2d(256, 256, kernel_size = 3, padding = 1)\n",
    "        self.bn4_8 = nn.BatchNorm2d(256)\n",
    "        self.relu4_8 = nn.ReLU()\n",
    "        \n",
    "        self.conv4_9 = nn.Conv2d(256, 256, kernel_size = 3, padding = 1)\n",
    "        self.bn4_9 = nn.BatchNorm2d(256)\n",
    "        self.relu4_9 = nn.ReLU()\n",
    "        \n",
    "        self.conv4_10 = nn.Conv2d(256, 256, kernel_size = 3, padding = 1)\n",
    "        self.bn4_10 = nn.BatchNorm2d(256)\n",
    "        self.relu4_10 = nn.ReLU()\n",
    "        \n",
    "        self.conv4_11 = nn.Conv2d(256, 256, kernel_size = 3, padding = 1)\n",
    "        self.bn4_11 = nn.BatchNorm2d(256)\n",
    "        self.relu4_11 = nn.ReLU()\n",
    "        \n",
    "        self.conv4_12 = nn.Conv2d(256, 256, kernel_size = 3, padding = 1)\n",
    "        self.bn4_12 = nn.BatchNorm2d(256)\n",
    "        self.relu4_12 = nn.ReLU()\n",
    "        \n",
    "        # 512 channels\n",
    "        self.conv5_1 = nn.Conv2d(256, 512, kernel_size = 3, stride = 2)\n",
    "        self.bn5_1 = nn.BatchNorm2d(512)\n",
    "        self.relu5_1 = nn.ReLU()\n",
    "        \n",
    "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size = 3, padding = 1)\n",
    "        self.bn5_2 = nn.BatchNorm2d(512)\n",
    "        self.relu5_2 = nn.ReLU()\n",
    "        \n",
    "        self.conv5_proj = nn.Conv2d(256, 512, kernel_size = 1) # for projection shortcut\n",
    "        \n",
    "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size = 3, padding = 1)\n",
    "        self.bn5_3 = nn.BatchNorm2d(512)\n",
    "        self.relu5_3 = nn.ReLU()\n",
    "        \n",
    "        self.conv5_4 = nn.Conv2d(512, 512, kernel_size = 3, padding = 1)\n",
    "        self.bn5_4 = nn.BatchNorm2d(512)\n",
    "        self.relu5_4 = nn.ReLU()\n",
    "        \n",
    "        self.conv5_5 = nn.Conv2d(512, 512, kernel_size = 3, padding = 1)\n",
    "        self.bn5_5 = nn.BatchNorm2d(512)\n",
    "        self.relu5_5 = nn.ReLU()\n",
    "        \n",
    "        self.conv5_6 = nn.Conv2d(512, 512, kernel_size = 3, padding = 1)\n",
    "        self.bn5_6 = nn.BatchNorm2d(512)\n",
    "        self.relu5_6 = nn.ReLU()\n",
    "        \n",
    "        # global avg pooling\n",
    "        self.Adaavgpool1 = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # FC layer\n",
    "        self.fc1 = nn.Linear(512, 1000)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        # 64 channels\n",
    "        identity = x # input 기록\n",
    "        \n",
    "        x = self.conv2_1(x)\n",
    "        x = self.bn2_1(x)\n",
    "        x = self.relu2_1(x)\n",
    "        \n",
    "        x = self.conv2_2(x)\n",
    "        x = self.bn2_2(x)\n",
    "        x = self.relu2_2(x) + identity # 차원이 동일\n",
    "        \n",
    "        identity = x # input 기록\n",
    "        \n",
    "        x = self.conv2_3(x)\n",
    "        x = self.bn2_3(x)\n",
    "        x = self.relu2_3(x)\n",
    "        \n",
    "        x = self.conv2_4(x)\n",
    "        x = self.bn2_4(x)\n",
    "        x = self.relu2_4(x) + identity\n",
    "        \n",
    "        identity = x # input 기록\n",
    "        \n",
    "        x = self.conv2_5(x)\n",
    "        x = self.bn2_5(x)\n",
    "        x = self.relu2_5(x)\n",
    "        \n",
    "        x = self.conv2_6(x)\n",
    "        x = self.bn2_6(x)\n",
    "        x = self.relu2_6(x)\n",
    "        \n",
    "        identity = x # input 기록\n",
    "        \n",
    "        # 128 channels\n",
    "        x = self.conv3_1(x)\n",
    "        x = self.bn3_1(x)\n",
    "        x = self.relu3_1(x)\n",
    "        \n",
    "        x = self.conv3_2(x)\n",
    "        x = self.bn3_2(x)\n",
    "        y = self.conv3_proj(identity) # 1x1 conv filter\n",
    "        x = self.relu3_2(x + y)\n",
    "        \n",
    "        identity = x # input 기록\n",
    "        \n",
    "        x = self.conv3_3(x)\n",
    "        x = self.bn3_3(x)\n",
    "        x = self.relu3_3(x)\n",
    "        \n",
    "        x = self.conv3_4(x)\n",
    "        x = self.bn3_4(x)\n",
    "        x = self.relu3_4(x) + identity\n",
    "        \n",
    "        identity = x # input 기록\n",
    "        \n",
    "        x = self.conv3_5(x)\n",
    "        x = self.bn3_5(x)\n",
    "        x = self.relu3_5(x)\n",
    "        \n",
    "        x = self.conv3_6(x)\n",
    "        x = self.bn3_6(x)\n",
    "        x = self.relu3_6(x) + identity\n",
    "        \n",
    "        identity = x # input 기록\n",
    "        \n",
    "        x = self.conv3_7(x)\n",
    "        x = self.bn3_7(x)\n",
    "        x = self.relu3_7(x)\n",
    "        \n",
    "        x = self.conv3_8(x)\n",
    "        x = self.bn3_8(x)\n",
    "        x = self.relu3_8(x) + identity\n",
    "        \n",
    "        identity = x # input 기록\n",
    "        \n",
    "        # 256 channels\n",
    "        x = self.conv4_1(x)\n",
    "        x = self.bn4_1(x)\n",
    "        x = self.relu4_1(x)\n",
    "        \n",
    "        x = self.conv4_2(x)\n",
    "        x = self.bn4_2(x)\n",
    "        y = self.conv4_proj(identity) # 1x1 filter 256개 사용\n",
    "        x = self.relu4_2(x + y)\n",
    "        \n",
    "        identity = x\n",
    "        \n",
    "        x = self.conv4_3(x)\n",
    "        x = self.bn4_3(x)\n",
    "        x = self.relu4_3(x)\n",
    "        \n",
    "        x = self.conv4_4(x)\n",
    "        x = self.bn4_4(x)\n",
    "        x = self.relu4_4(x) + identity\n",
    "        \n",
    "        identity = x\n",
    "        \n",
    "        x = self.conv4_5(x)\n",
    "        x = self.bn4_5(x)\n",
    "        x = self.relu4_5(x)\n",
    "        \n",
    "        x = self.conv4_6(x)\n",
    "        x = self.bn4_6(x)\n",
    "        x = self.relu4_6(x) + identity\n",
    "        \n",
    "        identity = x\n",
    "        \n",
    "        x = self.conv4_7(x)\n",
    "        x = self.bn4_7(x)\n",
    "        x = self.relu4_7(x)\n",
    "        \n",
    "        x = self.conv4_8(x)\n",
    "        x = self.bn4_8(x)\n",
    "        x = self.relu4_8(x) + identity\n",
    "        \n",
    "        identity = x\n",
    "        \n",
    "        x = self.conv4_9(x)\n",
    "        x = self.bn4_9(x)\n",
    "        x = self.relu4_9(x)\n",
    "        \n",
    "        x = self.conv4_10(x)\n",
    "        x = self.bn4_10(x)\n",
    "        x = self.relu4_10(x) + identity\n",
    "        \n",
    "        identity = x\n",
    "        \n",
    "        x = self.conv4_11(x)\n",
    "        x = self.bn4_11(x)\n",
    "        x = self.relu4_11(x)\n",
    "        \n",
    "        x = self.conv4_12(x)\n",
    "        x = self.bn4_12(x)\n",
    "        x = self.relu4_12(x) + identity\n",
    "        \n",
    "        identity = x\n",
    "        \n",
    "        # 512 channels\n",
    "        x = self.conv5_1(x)\n",
    "        x = self.bn5_1(x)\n",
    "        x = self.relu5_1(x)\n",
    "        \n",
    "        x = self.conv5_2(x)\n",
    "        x = self.bn5_2(x)\n",
    "        y = self.conv5_proj(identity)\n",
    "        x = self.relu5_2(x + y)\n",
    "        \n",
    "        identity = x\n",
    "        \n",
    "        x = self.conv5_3(x)\n",
    "        x = self.bn5_3(x)\n",
    "        x = self.relu5_3(x)\n",
    "        \n",
    "        x = self.conv5_4(x)\n",
    "        x = self.bn5_4(x)\n",
    "        x = self.relu5_4(x) + identity\n",
    "        \n",
    "        identity = x\n",
    "        \n",
    "        x = self.conv5_5(x)\n",
    "        x = self.bn5_5(x)\n",
    "        x = self.relu5_5(x)\n",
    "        \n",
    "        x = self.conv5_6(x)\n",
    "        x = self.bn5_6(x)\n",
    "        x = self.relu5_6(x) + identity\n",
    "        \n",
    "        # global avg pool 2d\n",
    "        x = self.Adaavgpool1(x)\n",
    "        \n",
    "        # Linear\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
